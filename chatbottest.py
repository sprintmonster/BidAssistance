# -*- coding: utf-8 -*-
"""ë¹…í”„ llm íŒŒì´í”„ë¼ì¸

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sxX__wd30bLxV1NL1MaipW76KO03VlKJ
"""

!pip install fastapi uvicorn pyngrok



# llm.py
import os

USE_OPENAI = True  # ğŸ” Trueë¡œ ë°”ê¾¸ë©´ ì‹¤ì œ LLM ì‚¬ìš©

def call_llm(prompt: str) -> str:
    if USE_OPENAI:
        from openai import OpenAI
        client = OpenAI(api_key=os.environ[""])

        resp = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            temperature=0
        )
        return resp.choices[0].message.content.strip()

    # âœ… Mock LLM (PoCìš©)
    if "Classify" in prompt:
        if "ìš”ì•½" in prompt:
            return "report_summary"
        if "ë³´ê³ ì„œ" in prompt:
            return "report_qa"
        if "ê¸°ëŠ¥" in prompt:
            return "site_feature_qa"
        return "general_chat"

    return "ì´ê²ƒì€ LLMì˜ ì‘ë‹µ ì˜ˆì‹œì…ë‹ˆë‹¤."

# intent_router.py
#from llm import call_llm

INTENT_PROMPT = """
Classify the user request into one of:
- general_chat
- report_summary
- report_qa
- site_feature_qa

Return only the label.
"""

def route_intent(query: str) -> str:
    result = call_llm(INTENT_PROMPT + "\nUser: " + query)
    return result.strip()

# rag.py
def summarize_report():
    return "ğŸ“„ ë³´ê³ ì„œ ìš”ì•½ì…ë‹ˆë‹¤: ì´ ë³´ê³ ì„œëŠ” ë§¤ì¶œ ì¦ê°€ì™€ ë¦¬ìŠ¤í¬ë¥¼ ë‹¤ë£¹ë‹ˆë‹¤."

def qa_report(query: str):
    return f"ğŸ“Œ ë³´ê³ ì„œì—ì„œ '{query}'ì— ëŒ€í•œ ë‹µë³€ì…ë‹ˆë‹¤."

# site_qa.py
def site_feature_answer(query: str):
    return "ğŸ§­ ì´ ì‚¬ì´íŠ¸ëŠ” ë³´ê³ ì„œ ìš”ì•½, ì§ˆì˜ì‘ë‹µ, ì±—ë´‡ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤."

# app.py
from fastapi import FastAPI
from pydantic import BaseModel
'''
from intent_router import route_intent
from rag import summarize_report, qa_report
from site_qa import site_feature_answer
from llm import call_llm
'''
app = FastAPI(title="LLM Pipeline PoC")

class ChatRequest(BaseModel):
    query: str

@app.get("/")
def root():
    return {"status": "running"}

@app.post("/chat")
def chat(req: ChatRequest):
    intent = route_intent(req.query)

    if intent == "report_summary":
        answer = summarize_report()

    elif intent == "report_qa":
        answer = qa_report(req.query)

    elif intent == "site_feature_qa":
        answer = site_feature_answer(req.query)

    else:
        answer = call_llm(req.query)

    return {
        "intent": intent,
        "answer": answer
    }

from fastapi import FastAPI
from pydantic import BaseModel

app = FastAPI(title="LLM Pipeline PoC")

class ChatRequest(BaseModel):
    query: str

@app.get("/")
def root():
    return {"status": "running"}

@app.post("/chat")
def chat(req: ChatRequest):
    query = req.query
    intent = route_intent(query)

    if intent == "report_summary":
        answer = summarize_report()

    elif intent == "report_qa":
        answer = qa_report(query)

    elif intent == "site_feature_qa":
        answer = site_feature_answer(query)

    else:
        answer = call_llm(query)

    return {
        "intent": intent,
        "answer": answer
    }

from fastapi.middleware.cors import CORSMiddleware
# ëª¨ë“  ì¶œì²˜(origin)ì—ì„œì˜ ì ‘ê·¼ì„ í—ˆìš©í•˜ê¸° ìœ„í•œ ì„¤ì •
origins = ["*"]

# CORS(Cross-Origin Resource Sharing) ë¯¸ë“¤ì›¨ì–´ ì¶”ê°€
app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,        # ëª¨ë“  ë„ë©”ì¸ì—ì„œì˜ ìš”ì²­ í—ˆìš©
    allow_credentials=True,     # ì¿ í‚¤, ì¸ì¦ ì •ë³´ í¬í•¨ ìš”ì²­ í—ˆìš©
    allow_methods=["GET", "POST", "PUT", "DELETE"],  # í—ˆìš©í•  HTTP ë©”ì„œë“œ
    allow_headers=origins,        # ëª¨ë“  HTTP í—¤ë” í—ˆìš©
)

from pyngrok import ngrok
import nest_asyncio
from uvicorn import Config, Server

#from app import app

nest_asyncio.apply()

ngrok.set_auth_token("")
public_url = ngrok.connect(8000)
print("ğŸŒ Public URL:", public_url)

config = Config(app=app, host="0.0.0.0", port=8000)
server = Server(config)

await server.serve()

from fastapi import FastAPI
from pydantic import BaseModel

# ------------------
# Mock LLM
# ------------------
from openai import OpenAI
os.environ["OPENAI_API_KEY"] =""
api_key = os.getenv("OPENAI_API_KEY")
client = OpenAI(api_key=api_key)

def call_llm(prompt: str) -> str:
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": "You are an intent classifier and assistant."},
            {"role": "user", "content": prompt}
        ],
        temperature=0
    )
    return response.choices[0].message.content.strip()

# ------------------
# Intent Router
# ------------------
def route_intent(query: str) -> str:
    prompt = f"""
    Classify the user request into one of:
    - general_chat
    - report_summary
    - report_qa
    - site_feature_qa

    User: {query}
    """
    return call_llm(prompt)

# ------------------
# RAG / Site QA
# ------------------
def summarize_report():
    return "ğŸ“„ ë³´ê³ ì„œ ìš”ì•½ì…ë‹ˆë‹¤."

def qa_report(query: str):
    return f"ğŸ“Œ ë³´ê³ ì„œ ì§ˆì˜ ì‘ë‹µ: {query}"

def site_feature_answer(query: str):
    return "ğŸ§­ ì´ ì‚¬ì´íŠ¸ëŠ” ë³´ê³ ì„œ ìš”ì•½ê³¼ ì§ˆì˜ì‘ë‹µì„ ì œê³µí•©ë‹ˆë‹¤."

# ------------------
# FastAPI App
# ------------------
app = FastAPI()

class ChatRequest(BaseModel):
    query: str

@app.get("/")
def root():
    return {"status": "running"}

@app.post("/chat")
def chat(req: ChatRequest):
    intent = route_intent(req.query)

    if intent == "report_summary":
        answer = summarize_report()
    elif intent == "report_qa":
        answer = qa_report(req.query)
    elif intent == "site_feature_qa":
        answer = site_feature_answer(req.query)
    else:
        answer = call_llm(req.query)

    return {
        "intent": intent,
        "answer": answer
    }

from pyngrok import ngrok
import nest_asyncio
from uvicorn import Config, Server

nest_asyncio.apply()

ngrok.set_auth_token("")
print(ngrok.connect(8000))

config = Config(app=app, host="0.0.0.0", port=8000)
server = Server(config)

await server.serve()