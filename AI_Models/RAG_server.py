import uvicorn
import torch
import re
import nest_asyncio
import os
import json
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
from typing import List, Dict, Any, Optional

# --- ëª¨ë“ˆ ì„í¬íŠ¸ ---
# íŒŒì¼ë“¤ì´ ê°™ì€ ë””ë ‰í† ë¦¬ì— ìˆë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.
try:
    from model_transformer import TransformerRegressor
    from BidAssitanceModel import BidRAGPipeline
except ImportError as e:
    print(f"âŒ í•„ìˆ˜ ëª¨ë“ˆ ë¡œë”© ì‹¤íŒ¨: {e}")
    print("model_transformer.py ì™€ BidAssitanceModel.py íŒŒì¼ì´ í•„ìš”í•©ë‹ˆë‹¤.")
    exit(1)

from pyngrok import ngrok

# ==========================================
# 0. ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ (parsenumber ì§ì ‘ êµ¬í˜„)
# ==========================================
def parsenumber(value: Any) -> Optional[float]:
    """
    ë‹¤ì–‘í•œ í˜•íƒœì˜ ìˆ«ì ë¬¸ìì—´ì„ floatë¡œ ë³€í™˜ (BidAssitanceModel.py ë¡œì§ ë³µì‚¬)
    ì˜ˆ: "1,000,000ì›" -> 1000000.0
    """
    if value is None:
        return None
    if isinstance(value, (int, float)):
        return float(value)
    
    s = str(value).strip()
    if not s:
        return None
        
    # í†µí™” ê¸°í˜¸ ë° ì½¤ë§ˆ ì œê±°
    s = s.replace(',', '').replace('ì›', '').replace('KRW', '').replace('â‚©', '')
    
    # ìˆ«ì, ì (.), ë§ˆì´ë„ˆìŠ¤(-) ì™¸ì˜ ë¬¸ì ì œê±°
    s = re.sub(r'[^0-9.\-]', '', s)
    
    if not s or s in ('-', '.', '-.'):
        return None
        
    try:
        return float(s)
    except Exception:
        return None

def load_transformer_model(model_path: str):
    print(f"ğŸ”„ ëª¨ë¸ ë¡œë”© ì¤‘: {model_path}")
    
    if not os.path.exists(model_path):
        print("âš ï¸ ëª¨ë¸ íŒŒì¼ ì—†ìŒ. ê¸°ë³¸ê°’ ì‚¬ìš©.")
        return None, {"num_features": 4, "d_model": 512} # ë”ë¯¸ ë°˜í™˜

    state_dict = torch.load(model_path, map_location='cpu')

    # --- [ìë™ ê°ì§€ ì‹œì‘] ---
    config = {
        "num_features": 4,   # ê¸°ë³¸ê°’ (ê°ì§€ ì‹¤íŒ¨ ì‹œ)
        "d_model": 128,      # ê¸°ë³¸ê°’
        "num_layers": 2,     # ê¸°ë³¸ê°’
        "dim_feedforward": 512, # ê¸°ë³¸ê°’
        "nhead": 4           # ê¸°ë³¸ê°’ (weight shapeë§Œìœ¼ë¡œëŠ” ì•Œ ìˆ˜ ì—†ìŒ)
    }

    # 1. d_model & num_features ê°ì§€ (cls_token ë˜ëŠ” ì²« ë ˆì´ì–´)
    if 'cls_token' in state_dict:
        # cls_token shape: [1, 1, d_model]
        config['d_model'] = state_dict['cls_token'].shape[2]
        
    for key, param in state_dict.items():
        # feature_emb.weight shape: [num_features, d_model] (ë˜ëŠ” ë°˜ëŒ€)
        # í•˜ì§€ë§Œ ë³´í†µ ì‘ì€ ê°’ì´ num_featuresì´ë¯€ë¡œ min/maxë¡œ êµ¬ë¶„ ê°€ëŠ¥
        if 'feature_emb.weight' in key:
            # ì˜ˆ: [4, 128] -> 4ê°€ feature, 128ì´ d_model
            dim1, dim2 = param.shape
            config['num_features'] = min(dim1, dim2)
            # d_modelì€ ìœ„ì—ì„œ cls_tokenìœ¼ë¡œ ì°¾ì€ ê±¸ ì‹ ë¢°í•˜ê±°ë‚˜, í° ê°’ì„ ì‚¬ìš©
            if 'cls_token' not in state_dict:
                 config['d_model'] = max(dim1, dim2)
            break

    # 2. dim_feedforward ê°ì§€ (linear1ì˜ ì¶œë ¥ í¬ê¸°)
    # ë³´í†µ 'encoder.layers.0.linear1.weight' í˜•íƒœë¡œ ì €ì¥ë¨
    for key, param in state_dict.items():
        if 'linear1.weight' in key:
            # Linear(d_model, dim_feedforward) -> weight shape: [dim_ff, d_model]
            # ë”°ë¼ì„œ shape[0]ì´ dim_feedforward
            config['dim_feedforward'] = param.shape[0]
            print(f"ğŸ” Feedforward ì°¨ì› ê°ì§€ë¨: {config['dim_feedforward']}")
            break

    # 3. num_layers ê°ì§€
    max_layer_idx = -1
    for key in state_dict.keys():
        match = re.search(r'layers\.(\d+)\.', key)
        if match:
            max_layer_idx = max(max_layer_idx, int(match.group(1)))
    if max_layer_idx != -1:
        config['num_layers'] = max_layer_idx + 1

    print(f"âœ… ìµœì¢… ìë™ ê°ì§€ ì„¤ì •: {config}")
    # -----------------------

    # ëª¨ë¸ ì´ˆê¸°í™”
    model = TransformerRegressor(
        num_features=config['num_features'],
        d_model=config['d_model'],
        num_layers=config['num_layers'],
        nhead=config['nhead'], # nheadëŠ” ê°ì§€ ë¶ˆê°€ (ë³´í†µ 4 or 8)
        dim_feedforward=config['dim_feedforward'], # â˜… ìë™ ê°ì§€ëœ ê°’ ì ìš©
        dropout=0.1
    )
    
    try:
        model.load_state_dict(state_dict, strict=False)
        print("ğŸ‰ ëª¨ë¸ íŒŒë¼ë¯¸í„° ë¡œë“œ ì„±ê³µ!")
    except Exception as e:
        print(f"âŒ ë¡œë“œ ì‹¤íŒ¨: {e}")
        # ì‹¤íŒ¨ ì‹œ ë¹ˆ ëª¨ë¸ ë°˜í™˜í•˜ì§€ë§Œ configëŠ” ìœ ì§€
    
    model.eval()
    return model, config



# ëª¨ë¸ ê²½ë¡œ ì„¤ì • (ì‹¤ì œ ê²½ë¡œë¡œ ìˆ˜ì • í•„ìš”)
MODEL_PATH = "../results_transformer/best_model.pt"
TF_MODEL, TF_CONFIG = load_transformer_model(MODEL_PATH)


# ==========================================
# 2. RAG íŒŒì´í”„ë¼ì¸ ì–´ëŒ‘í„°
# ==========================================
class TransformerPredictorAdapter:
    def __init__(self, model, input_dim):
        self.model = model
        self.input_dim = input_dim

    def predict(self, requirements: Dict[str, Any], retrieved_context: str) -> Dict[str, Any]:
        try:
            # ì¶”ì¶œëœ ì •ë³´ íŒŒì‹±
            budget = parsenumber(requirements.get('budget'))
            estimate = parsenumber(requirements.get('estimate_price'))
            # ì˜ˆê°€ë²”ìœ„, ë‚™ì°°í•˜í•œìœ¨ì€ ë°±ë¶„ìœ¨ì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì¶”ê°€ ì²˜ë¦¬ í•„ìš”í•  ìˆ˜ ìˆìŒ
            pr_range = parsenumber(requirements.get('expected_price_range'))
            lower_rate = parsenumber(requirements.get('award_lower_rate'))

            # Noneì´ë©´ 0.0ìœ¼ë¡œ ëŒ€ì²´
            features = [
                budget if budget else 0.0,
                estimate if estimate else 0.0,
                pr_range if pr_range else 0.0,
                lower_rate if lower_rate else 0.0
            ]

            # ëª¨ë¸ ì¶”ë¡ 
            if self.model:
                input_tensor = torch.tensor([features], dtype=torch.float32)
                with torch.no_grad():
                    pred_raw = self.model(input_tensor).item()
            else:
                pred_raw = 0.0 # ëª¨ë¸ ì—†ì„ ë•Œ

            return {
                "currency": "KRW",
                "point_estimate": round(pred_raw),
                "predicted_min": round(pred_raw * 0.98), # ë‹¨ìˆœ ì˜ˆì‹œ ë²”ìœ„
                "predicted_max": round(pred_raw * 1.02),
                "confidence": "high" if self.model else "low",
                "rationale": f"Transformer Model (Inputs: {features})",
                "model_type": "TransformerRegressor"
            }
        except Exception as e:
            return {"error": str(e), "rationale": "Prediction Failed"}

# ì–´ëŒ‘í„° ë° íŒŒì´í”„ë¼ì¸ ìƒì„±
adapter = TransformerPredictorAdapter(TF_MODEL, TF_CONFIG['num_features'])

print("ğŸš€ RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”...")
# ë¬¸ì„œ/ì¸ë±ìŠ¤ ê²½ë¡œëŠ” ì‹¤ì œ í™˜ê²½ì— ë§ê²Œ ìˆ˜ì •í•´ì£¼ì„¸ìš”
rag_pipeline = BidRAGPipeline(
    doc_dir="./rag_corpus", 
    index_dir="./rag_index",
    award_predict_fn=adapter.predict # â˜… ì–´ëŒ‘í„° í•¨ìˆ˜ ì£¼ì…
)

# ==========================================
# 3. FastAPI ì„œë²„
# ==========================================
app = FastAPI(title="Integrated Bid Prediction API")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

class PredictReq(BaseModel):
    features: List[float]

class AnalyzeReq(BaseModel):
    text: str
    thread_id: str = "default"

@app.post("/predictBase")
async def predict_base(req: PredictReq):
    if not TF_MODEL:
        return {"error": "Model not loaded", "predBid": 0}
        
    try:
        input_tensor = torch.tensor([req.features], dtype=torch.float32)
        with torch.no_grad():
            pred = TF_MODEL(input_tensor).item()
        return {"predBid": pred}
    except Exception as e:
        return {"error": str(e), "predBid": 0}

@app.post("/analyze")
async def analyze(req: AnalyzeReq):
    try:
        # RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        result = rag_pipeline.analyze(req.text, thread_id=req.thread_id)
        
        # ê²°ê³¼ ì •ë¦¬
        return {
            "extracted_requirements": result.get("requirements", {}),
            "prediction": result.get("prediction_result", {}),
            "report": result.get("report_markdown", "")
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/")
def root():
    return {"status": "running"}

if __name__ == "__main__":
    auth_token = "38H6WIHF5Hn1xV68lPnXu15Tutc_4PDGKRtxpJhbJuVdcUCEp"
    ngrok.set_auth_token(auth_token)
    url = ngrok.connect(9999).public_url
    print(f"ğŸŒ Public URL: {url}")
    
    nest_asyncio.apply()
    uvicorn.run(app, host="0.0.0.0", port=9999)
