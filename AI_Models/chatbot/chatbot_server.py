import os
import uvicorn
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from dotenv import load_dotenv
from pyngrok import ngrok
from openai import OpenAI

# ------------------
# 1. í™˜ê²½ ì„¤ì • (.env ë¡œë“œ)
# ------------------
# .env íŒŒì¼ ëª…ì‹œì  ë¡œë“œ (í˜„ì¬ ë””ë ‰í† ë¦¬ ê¸°ì¤€)
load_dotenv(verbose=True)

class Config:
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
    NGROK_AUTH_TOKEN = os.getenv("NGROK_AUTH_TOKEN")
    
    @classmethod
    def check(cls):
        if not cls.OPENAI_API_KEY:
            raise ValueError("âŒ Error: 'OPENAI_API_KEY'ê°€ .env íŒŒì¼ì— ì—†ìŠµë‹ˆë‹¤.")
        if not cls.NGROK_AUTH_TOKEN:
            print("âš ï¸ Warning: 'NGROK_AUTH_TOKEN'ì´ ì—†ì–´ ì™¸ë¶€ ì ‘ì†(Public URL)ì´ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤.")

# í•„ìˆ˜ í‚¤ ì²´í¬ ì‹¤í–‰
try:
    Config.check()
except ValueError as e:
    print(e)
    exit(1) # í‚¤ ì—†ìœ¼ë©´ ì¢…ë£Œ

# ------------------
# 2. LLM Client Setup
# ------------------
client = OpenAI(api_key=Config.OPENAI_API_KEY)

def call_llm(prompt: str, system_role: str = "You are a helpful assistant.") -> str:
    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_role},
                {"role": "user", "content": prompt}
            ],
            temperature=0
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        return f"Error calling LLM: {str(e)}"

# ------------------
# 3. Business Logic
# ------------------
def route_intent(query: str) -> str:
    prompt = f"""
    Classify the user request into exactly one of these labels:
    - general_chat
    - report_summary
    - report_qa
    - site_feature_qa

    Do not explain, just return the label.
    User: {query}
    """
    return call_llm(prompt, system_role="You are an intent classifier.")

def summarize_report():
    return "ğŸ“„ [Mock] ë³´ê³ ì„œ ìš”ì•½ì…ë‹ˆë‹¤: ì´ ë³´ê³ ì„œëŠ” ê¸ˆë…„ë„ ë§¤ì¶œ 20% ì¦ê°€ì™€ ì£¼ìš” ë¦¬ìŠ¤í¬ ìš”ì¸ì„ ë‹¤ë£¨ê³  ìˆìŠµë‹ˆë‹¤."

def qa_report(query: str):
    return f"ğŸ“Œ [Mock] ë³´ê³ ì„œ ë‚´ìš© ê¸°ë°˜ ë‹µë³€: '{query}'ì— ëŒ€í•œ ë¶„ì„ ê²°ê³¼ëŠ” 3í˜ì´ì§€ì— ìˆìŠµë‹ˆë‹¤."

def site_feature_answer(query: str):
    return "ğŸ§­ [Mock] ì‚¬ì´íŠ¸ ì•ˆë‚´: ì €í¬ ì‚¬ì´íŠ¸ëŠ” PDF ë³´ê³ ì„œ ìš”ì•½, AI ì§ˆì˜ì‘ë‹µ, ì‹¤ì‹œê°„ ì±„íŒ… ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤."

# ------------------
# 4. FastAPI App Setup
# ------------------
app = FastAPI(title="LLM Pipeline PoC")

# CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

class ChatRequest(BaseModel):
    query: str

@app.get("/status_check")
def root():
    return {"status": "running", "message": "API Active"}

@app.post("/chat")
def chat(req: ChatRequest):
    intent = route_intent(req.query)
    print(f"[Log] Intent: {intent}")

    if "report_summary" in intent:
        answer = summarize_report()
    elif "report_qa" in intent:
        answer = qa_report(req.query)
    elif "site_feature_qa" in intent:
        answer = site_feature_answer(req.query)
    else:
        answer = call_llm(req.query)

    return {"intent": intent, "answer": answer}

# ------------------
# 5. Server Execution
# ------------------
if __name__ == "__main__":
    # ngrok ì„¤ì • (í† í°ì´ .envì— ìˆì„ ë•Œë§Œ)
    if Config.NGROK_AUTH_TOKEN:
        ngrok.set_auth_token(Config.NGROK_AUTH_TOKEN)
        public_url = ngrok.connect(8000)
        print(f"\nğŸŒ Public URL: {public_url.public_url}\n")
    
    uvicorn.run(app, host="0.0.0.0", port=8000)
