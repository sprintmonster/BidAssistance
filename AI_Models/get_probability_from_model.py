import torch
import torch.nn as nn
import numpy as np
import os
import warnings
warnings.filterwarnings('ignore')

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')


class QuantileTransformerRegressor(nn.Module):
    """Quantile Regressionì„ ìœ„í•œ Transformer ê¸°ë°˜ ëª¨ë¸"""
    
    def __init__(self, input_dim, num_quantiles=999, d_model=128, nhead=8, 
                 num_layers=3, dim_feedforward=512, dropout=0.1):
        super(QuantileTransformerRegressor, self).__init__()
        self.num_quantiles = num_quantiles
        
        self.input_embedding = nn.Linear(input_dim, d_model)
        self.pos_encoder = nn.Parameter(torch.randn(1, 1, d_model))
        
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward,
            dropout=dropout, batch_first=True
        )
        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        
        self.fc_out = nn.Sequential(
            nn.Linear(d_model, dim_feedforward // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(dim_feedforward // 2, dim_feedforward // 4),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(dim_feedforward // 4, num_quantiles)
        )
        
    def forward(self, x):
        x = self.input_embedding(x)
        x = x.unsqueeze(1) + self.pos_encoder
        x = self.transformer_encoder(x)
        return self.fc_out(x.squeeze(1))


class ProbabilityPredictor:
    """TFT 4-Feature ëª¨ë¸ì„ ì‚¬ìš©í•œ í™•ë¥  ì˜ˆì¸¡ í´ë˜ìŠ¤"""
    
    def __init__(self, model_path='./results_tft_4feat/best_model.pt'):
        self.model_path = model_path
        self.device = device
        self.quantiles = np.linspace(0.001, 0.999, 999)
        self.feature_names = ['ì˜ˆê°€ë²”ìœ„', 'ë‚™ì°°í•˜í•œìœ¨', 'ì¶”ì •ê°€ê²©', 'ê¸°ì´ˆê¸ˆì•¡']
        self.model = self._load_model()
        self.scaler = None
        
    def _load_model(self):
        """í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ"""
        model = QuantileTransformerRegressor(
            input_dim=4, num_quantiles=999, d_model=128, nhead=8,
            num_layers=3, dim_feedforward=512, dropout=0.1
        ).to(self.device)
        
        if not os.path.exists(self.model_path):
            raise FileNotFoundError(f"ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.model_path}")
        
        checkpoint = torch.load(self.model_path, map_location=self.device)
        model.load_state_dict(checkpoint['model_state_dict'], strict=False)
        model.eval()
        
        print(f"âœ“ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {self.model_path}")
        if 'epoch' in checkpoint:
            print(f"  Epoch: {checkpoint['epoch']}, Val Loss: {checkpoint.get('val_loss', 0):.6f}")
        
        return model
    
    def _prepare_input(self, input_features):
        """ì…ë ¥ í”¼ì²˜ë¥¼ numpy arrayë¡œ ë³€í™˜"""
        if isinstance(input_features, dict):
            X = np.array([[
                input_features['ì˜ˆê°€ë²”ìœ„'],
                input_features['ë‚™ì°°í•˜í•œìœ¨'],
                input_features['ì¶”ì •ê°€ê²©'],
                input_features['ê¸°ì´ˆê¸ˆì•¡']
            ]], dtype=np.float32)
        else:
            X = np.array([input_features], dtype=np.float32)
            if X.shape[1] != 4:
                raise ValueError(f"ì…ë ¥ í”¼ì²˜ëŠ” 4ê°œì—¬ì•¼ í•©ë‹ˆë‹¤. í˜„ì¬: {X.shape[1]}ê°œ")
        
        if self.scaler is not None:
            X = self.scaler.transform(X)
        
        return X
    
    def _predict_quantiles(self, X):
        """999ê°œ quantile ì˜ˆì¸¡"""
        with torch.no_grad():
            X_tensor = torch.FloatTensor(X).to(self.device)
            return self.model(X_tensor).cpu().numpy()[0]
    
    def _get_input_features_dict(self, X):
        """ì…ë ¥ í”¼ì²˜ë¥¼ dict í˜•íƒœë¡œ ë°˜í™˜"""
        return {
            'ì˜ˆê°€ë²”ìœ„': float(X[0, 0]),
            'ë‚™ì°°í•˜í•œìœ¨': float(X[0, 1]),
            'ì¶”ì •ê°€ê²©': float(X[0, 2]),
            'ê¸°ì´ˆê¸ˆì•¡': float(X[0, 3])
        }
    
    def predict_probability(self, input_features, lower_bound, upper_bound):
        """íŠ¹ì • êµ¬ê°„ì˜ í™•ë¥  ì˜ˆì¸¡"""
        X = self._prepare_input(input_features)
        pred_quantiles = self._predict_quantiles(X)
        
        # êµ¬ê°„ ë‚´ í™•ë¥  ê³„ì‚°
        lower_idx = np.searchsorted(pred_quantiles, lower_bound, side='left')
        upper_idx = np.searchsorted(pred_quantiles, upper_bound, side='right')
        probability = (upper_idx - lower_idx) / len(pred_quantiles)
        
        return {
            'probability': float(probability),
            'probability_percent': float(probability * 100),
            'lower_bound': lower_bound,
            'upper_bound': upper_bound,
            'lower_quantile_index': int(lower_idx),
            'upper_quantile_index': int(upper_idx),
            'median_prediction': float(pred_quantiles[499]),
            'mean_prediction': float(np.mean(pred_quantiles)),
            'input_features': self._get_input_features_dict(X)
        }
    
    def get_prediction_intervals(self, input_features, confidence_levels=[0.5, 0.8, 0.9, 0.95]):
        """ì—¬ëŸ¬ ì‹ ë¢°êµ¬ê°„ ì˜ˆì¸¡"""
        X = self._prepare_input(input_features)
        pred_quantiles = self._predict_quantiles(X)
        
        intervals = {}
        for conf in confidence_levels:
            lower_idx = int((1 - conf) / 2 * 999)
            upper_idx = int((1 + conf) / 2 * 999)
            
            intervals[f'{int(conf*100)}%'] = {
                'lower': float(pred_quantiles[lower_idx]),
                'upper': float(pred_quantiles[upper_idx]),
                'median': float(pred_quantiles[499]),
                'width': float(pred_quantiles[upper_idx] - pred_quantiles[lower_idx])
            }
        
        return {
            'intervals': intervals,
            'median_prediction': float(pred_quantiles[499]),
            'mean_prediction': float(np.mean(pred_quantiles)),
            'input_features': self._get_input_features_dict(X)
        }
    
    
    def get_highest_probability_ranges(self, input_features, bin_width=0.001, top_k=3):
        """
        Quantile Functionì„ PDFë¡œ ë³€í™˜í•˜ì—¬ í™•ë¥  ë°€ë„ê°€ ë†’ì€ êµ¬ê°„ ì°¾ê¸°
        
        ìˆ˜í•™ì  ì›ë¦¬:
        - Quantile Function: Q(Ï„) = y, Ï„ âˆˆ [0.001, 0.999]
        - CDF: F(y) = Ï„ (ì—­í•¨ìˆ˜ ê´€ê³„)
        - PDF: f(y) = dF(y)/dy = dÏ„/dy
        
        ì´ì‚° ê·¼ì‚¬:
        - f(y_i) â‰ˆ Î”Ï„ / Î”Q = (Ï„_{i+1} - Ï„_{i-1}) / (Q_{i+1} - Q_{i-1})
        """
        X = self._prepare_input(input_features)
        pred_quantiles = self._predict_quantiles(X)  # Q(Ï„_i) for i=0..998
        
        # ğŸ” ë‹¨ì¡°ì„± ê²€ì‚¬
        non_monotonic = np.diff(pred_quantiles) < 0
        if np.any(non_monotonic):
            n_violations = np.sum(non_monotonic)
            print(f"âš ï¸  ê²½ê³ : Quantile Functionì´ {n_violations}ê°œ êµ¬ê°„ì—ì„œ ê°ì†Œí•©ë‹ˆë‹¤!")
            print(f"   ì´ëŠ” ì—­í•¨ìˆ˜ê°€ ì •ì˜ë˜ì§€ ì•ŠëŠ” êµ¬ê°„ì…ë‹ˆë‹¤.")
            violation_indices = np.where(non_monotonic)[0][:5]  # ì²˜ìŒ 5ê°œë§Œ
            for idx in violation_indices:
                print(f"   Ï„={self.quantiles[idx]:.3f}: Q={pred_quantiles[idx]:.4f} â†’ Q={pred_quantiles[idx+1]:.4f}")
        
        # 1. PDF ê³„ì‚°: f(y) = Î”Ï„ / Î”Q
        pdf_values = np.zeros(len(pred_quantiles))
        
        # ì¤‘ì‹¬ì°¨ë¶„ìœ¼ë¡œ PDF ê³„ì‚° (ì–‘ ë ì œì™¸)
        for i in range(1, len(pred_quantiles) - 1):
            delta_tau = self.quantiles[i+1] - self.quantiles[i-1]  # 0.002
            delta_Q = pred_quantiles[i+1] - pred_quantiles[i-1]
            
            if abs(delta_Q) > 1e-10:  # 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€
                pdf_values[i] = delta_tau / delta_Q
                # ìŒìˆ˜ PDF ë°©ì§€ (ë¹„ë‹¨ì¡° êµ¬ê°„)
                if pdf_values[i] < 0:
                    pdf_values[i] = 0  # ìŒìˆ˜ í™•ë¥ ë°€ë„ëŠ” 0ìœ¼ë¡œ ì²˜ë¦¬
            else:
                pdf_values[i] = 100.0  # ë§¤ìš° ë†’ì€ ë°€ë„ (í•˜ì§€ë§Œ í˜„ì‹¤ì ì¸ ê°’)
        
        # ì–‘ ëì  ì²˜ë¦¬ (ì „ì§„/í›„ì§„ ì°¨ë¶„)
        if len(pred_quantiles) > 1:
            # ì²« ì  (ì „ì§„ì°¨ë¶„)
            delta_tau_0 = self.quantiles[1] - self.quantiles[0]
            delta_Q_0 = pred_quantiles[1] - pred_quantiles[0]
            if abs(delta_Q_0) > 1e-10:
                pdf_values[0] = max(0, delta_tau_0 / delta_Q_0)  # ìŒìˆ˜ ë°©ì§€
            else:
                pdf_values[0] = 100.0
            
            # ë§ˆì§€ë§‰ ì  (í›„ì§„ì°¨ë¶„)
            delta_tau_last = self.quantiles[-1] - self.quantiles[-2]
            delta_Q_last = pred_quantiles[-1] - pred_quantiles[-2]
            if abs(delta_Q_last) > 1e-10:
                pdf_values[-1] = max(0, delta_tau_last / delta_Q_last)  # ìŒìˆ˜ ë°©ì§€
            else:
                pdf_values[-1] = 100.0
        
        # 2. bin_width ë‹¨ìœ„ë¡œ êµ¬ê°„ì„ ë‚˜ëˆ„ê³  í‰ê·  PDF ê³„ì‚°
        # min/maxë¥¼ bin_width ë‹¨ìœ„ë¡œ ì •ë ¬í•˜ì—¬ ê¹”ë”í•œ ê²½ê³„ ìƒì„±
        min_val = float(pred_quantiles.min())
        max_val = float(pred_quantiles.max())
        
        # bin_width ë‹¨ìœ„ë¡œ ë‚´ë¦¼/ì˜¬ë¦¼í•˜ì—¬ ì •ë°€ë„ ë§ì¶¤
        min_aligned = np.floor(min_val / bin_width) * bin_width
        max_aligned = np.ceil(max_val / bin_width) * bin_width
        
        bins = np.arange(min_aligned, max_aligned + bin_width, bin_width)
        
        bin_info = []
        for i in range(len(bins) - 1):
            lower, upper = bins[i], bins[i + 1]
            
            # ì´ êµ¬ê°„ì— ì†í•˜ëŠ” quantile ì°¾ê¸°
            in_bin = (pred_quantiles >= lower) & (pred_quantiles < upper if i < len(bins) - 2 else pred_quantiles <= upper)
            quantile_indices = np.where(in_bin)[0]
            
            if len(quantile_indices) == 0:
                continue
            
            # êµ¬ê°„ ë‚´ í‰ê·  PDF (í™•ë¥ ë°€ë„)
            avg_pdf = float(np.mean(pdf_values[quantile_indices]))
            
            # êµ¬ê°„ì˜ í™•ë¥  â‰ˆ âˆ« f(y) dy â‰ˆ f(y) Ã— Î”y
            probability = avg_pdf * bin_width
            
            bin_info.append({
                'range': f'{(lower-1)*100:+.1f}% ~ {(upper-1)*100:+.1f}%',  # ì¦ê°ìœ¼ë¡œ í‘œì‹œ, 0.1%p ë‹¨ìœ„, ~ ì•ë’¤ ê³µë°±
                'lower': float(lower),
                'upper': float(upper),
                'center': float((lower + upper) / 2),
                'pdf': avg_pdf,  # í™•ë¥ ë°€ë„ f(y)
                'probability': float(probability),  # P(y âˆˆ [lower, upper]) - ì •ê·œí™” ì „
                'probability_percent': float(probability * 100)
            })
        
        # ì „ì²´ í™•ë¥  ì •ê·œí™” (âˆ‘P = 1ì´ ë˜ë„ë¡)
        total_probability = sum(b['probability'] for b in bin_info)
        print(f"[DEBUG] ì •ê·œí™” ì „ total_probability: {total_probability:.4f}")
        
        if total_probability > 0:
            for b in bin_info:
                old_prob = b['probability']
                b['probability'] = b['probability'] / total_probability
                b['probability_percent'] = b['probability'] * 100
                if old_prob > 1.0:  # 100% ì´ˆê³¼í•œ êµ¬ê°„ë§Œ ì¶œë ¥
                    print(f"[DEBUG] êµ¬ê°„ [{b['lower']:.2f}, {b['upper']:.2f}]: {old_prob*100:.2f}% â†’ {b['probability_percent']:.2f}%")
        
        # PDF ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ (í™•ë¥ ë°€ë„ê°€ ë†’ì€ ìˆœ)
        sorted_bins = sorted(bin_info, key=lambda x: x['pdf'], reverse=True)
        
        return {
            'top_ranges': sorted_bins[:top_k],
            'all_ranges': sorted_bins,
            'total_bins': len(sorted_bins),
            'bin_width': bin_width,
            'prediction_range': {'min': min_val, 'max': max_val, 'range': max_val - min_val},
            'statistics': {
                'median': float(pred_quantiles[499]),
                'mean': float(np.mean(pred_quantiles)),
                'std': float(np.std(pred_quantiles)),
                'q25': float(pred_quantiles[249]),
                'q75': float(pred_quantiles[749])
            },
            'input_features': self._get_input_features_dict(X)
        }
    
    def get_most_probable_range(self, input_features, bin_width=0.5):
        """ê°€ì¥ í™•ë¥  ë°€ë„ê°€ ë†’ì€ êµ¬ê°„ 1ê°œ ë°˜í™˜"""
        result = self.get_highest_probability_ranges(input_features, bin_width, top_k=1)
        
        if not result['top_ranges']:
            return None
            
        most_probable = result['top_ranges'][0]
        return {
            'most_probable_range': most_probable['range'],
            'lower': most_probable['lower'],
            'upper': most_probable['upper'],
            'center': most_probable['center'],
            'probability': most_probable['probability'],
            'probability_percent': most_probable['probability_percent'],
            'statistics': result['statistics'],
            'prediction_range': result['prediction_range'],
            'input_features': result['input_features']
        }
    
    def get_mode_and_peak_density(self, input_features, bandwidth=0.001):
        """ìµœë¹ˆê°’(mode)ê³¼ peak ë°€ë„ ë¶„ì„"""
        X = self._prepare_input(input_features)
        pred_quantiles = self._predict_quantiles(X)
        
        # ë°€ë„ ê³„ì‚°
        densities = np.array([
            np.sum(np.abs(pred_quantiles - q_val) <= bandwidth) / 999 / (2 * bandwidth)
            for q_val in pred_quantiles
        ])
        
        # ìµœëŒ€ ë°€ë„ ì¸ë±ìŠ¤
        peak_idx = np.argmax(densities)
        mode_value = float(pred_quantiles[peak_idx])
        peak_lower, peak_upper = mode_value - bandwidth, mode_value + bandwidth
        peak_count = np.sum((pred_quantiles >= peak_lower) & (pred_quantiles <= peak_upper))
        
        return {
            'mode': mode_value,
            'mode_quantile': float(self.quantiles[peak_idx]),
            'peak_density': float(densities[peak_idx]),
            'peak_range': {
                'lower': float(peak_lower),
                'upper': float(peak_upper),
                'probability': float(peak_count / 999),
                'probability_percent': float(peak_count / 999 * 100)
            },
            'median': float(pred_quantiles[499]),
            'mean': float(np.mean(pred_quantiles)),
            'std': float(np.std(pred_quantiles)),
            'input_features': self._get_input_features_dict(X)
        }
    
    def evaluate_highest_probability_average(self, test_data_path='../dataset/dataset_feature_selected.csv', bin_width=0.001, max_samples=None):
        """
        í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ ëª¨ë“  ìƒ˜í”Œì— ëŒ€í•´ ìµœëŒ€ í™•ë¥  êµ¬ê°„ì˜ í‰ê· ê°’ìœ¼ë¡œ ì˜ˆì¸¡í•˜ê³  ì˜¤ì°¨ìœ¨ ê³„ì‚°
        
        Args:
            test_data_path: í…ŒìŠ¤íŠ¸ ë°ì´í„° CSV íŒŒì¼ ê²½ë¡œ
            bin_width: êµ¬ê°„ í­ (ê¸°ë³¸ê°’ 0.001 = 0.1%p)
            max_samples: ì²˜ë¦¬í•  ìµœëŒ€ ìƒ˜í”Œ ìˆ˜ (Noneì´ë©´ ì „ì²´)
        
        Returns:
            dict: í‰ê·  ì˜¤ì°¨ìœ¨ ë° ìƒì„¸ í†µê³„
        """
        import pandas as pd
        
        print(f"\n{'='*80}")
        print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„° í‰ê°€: ìµœëŒ€ í™•ë¥  êµ¬ê°„ì˜ í‰ê· ê°’ìœ¼ë¡œ ì˜ˆì¸¡")
        print(f"{'='*80}\n")
        
        # ë°ì´í„° ë¡œë“œ
        df = pd.read_csv(test_data_path)
        print(f"âœ“ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ: {len(df)}ê°œ ìƒ˜í”Œ")
        
        # ìƒ˜í”Œ ìˆ˜ ì œí•œ
        if max_samples is not None and len(df) > max_samples:
            df = df.head(max_samples)
            print(f"  ì²˜ë¦¬í•  ìƒ˜í”Œ ìˆ˜ ì œí•œ: {max_samples}ê°œ")
        
        print(f"  ì»¬ëŸ¼: {list(df.columns)}")
        print(f"  ì²˜ë¦¬í•  ìƒ˜í”Œ ìˆ˜: {len(df)}ê°œ\n")
        
        # ì‹¤ì œ ì‚¬ì •ìœ¨ì€ ë°ì´í„°ì…‹ì— ì´ë¯¸ ì¡´ì¬í•˜ëŠ” 'ì‚¬ì •ìœ¨' ì»¬ëŸ¼ ì‚¬ìš©
        if 'ì‚¬ì •ìœ¨' not in df.columns:
            raise ValueError("ë°ì´í„°ì…‹ì— 'ì‚¬ì •ìœ¨' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        
        results = []
        errors = []
        
        print("ìƒ˜í”Œë³„ ì˜ˆì¸¡ ì‹œì‘...")
        for idx, row in df.iterrows():
            # ì…ë ¥ í”¼ì²˜ ì¤€ë¹„
            input_features = {
                'ì˜ˆê°€ë²”ìœ„': row['ì˜ˆê°€ë²”ìœ„'],
                'ë‚™ì°°í•˜í•œìœ¨': row['ë‚™ì°°í•˜í•œìœ¨'],
                'ì¶”ì •ê°€ê²©': row['ì¶”ì •ê°€ê²©'],
                'ê¸°ì´ˆê¸ˆì•¡': row['ê¸°ì´ˆê¸ˆì•¡']
            }
            
            # ìµœëŒ€ í™•ë¥  êµ¬ê°„ ì°¾ê¸°
            result = self.get_highest_probability_ranges(input_features, bin_width=bin_width, top_k=1)
            
            if not result['top_ranges']:
                print(f"  ê²½ê³ : ìƒ˜í”Œ {idx}ì—ì„œ êµ¬ê°„ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                continue
            
            # ìµœëŒ€ í™•ë¥  êµ¬ê°„ì˜ ì¤‘ì‹¬ê°’ (í‰ê· )
            top_range = result['top_ranges'][0]
            predicted_rate = top_range['center']  # êµ¬ê°„ì˜ í‰ê· ê°’
            actual_rate = row['ì‚¬ì •ìœ¨']
            
            # ì˜¤ì°¨ ê³„ì‚°
            error = abs(predicted_rate - actual_rate)
            error_percent = error * 100  # %p ë‹¨ìœ„
            relative_error = (error / actual_rate) * 100  # ìƒëŒ€ ì˜¤ì°¨ (%)
            
            results.append({
                'index': idx,
                'actual_rate': actual_rate,
                'predicted_rate': predicted_rate,
                'error': error,
                'error_percent': error_percent,
                'relative_error': relative_error,
                'probability': top_range['probability_percent'],
                'range_lower': top_range['lower'],
                'range_upper': top_range['upper']
            })
            
            errors.append(error)
            
            # 100ê°œë§ˆë‹¤ ì§„í–‰ìƒí™© ì¶œë ¥
            if (idx + 1) % 100 == 0:
                print(f"  ì§„í–‰: {idx + 1}/{len(df)} ìƒ˜í”Œ ì²˜ë¦¬ ì™„ë£Œ...")
        
        # í†µê³„ ê³„ì‚°
        errors = np.array(errors)
        error_percents = errors * 100
        
        statistics = {
            'total_samples': len(results),
            'mean_absolute_error': float(np.mean(errors)),
            'mean_absolute_error_percent': float(np.mean(error_percents)),
            'median_absolute_error': float(np.median(errors)),
            'median_absolute_error_percent': float(np.median(error_percents)),
            'std_error': float(np.std(errors)),
            'std_error_percent': float(np.std(error_percents)),
            'min_error': float(np.min(errors)),
            'max_error': float(np.max(errors)),
            'q25_error': float(np.percentile(errors, 25)),
            'q75_error': float(np.percentile(errors, 75)),
            'mean_relative_error_percent': float(np.mean([r['relative_error'] for r in results]))
        }
        
        # ê²°ê³¼ ì¶œë ¥
        print(f"\n{'='*80}")
        print(f"í‰ê°€ ê²°ê³¼")
        print(f"{'='*80}")
        print(f"ì´ ìƒ˜í”Œ ìˆ˜: {statistics['total_samples']}")
        print(f"\n[ì ˆëŒ€ ì˜¤ì°¨ (ì‚¬ì •ìœ¨ ì°¨ì´)]")
        print(f"  í‰ê·  ì˜¤ì°¨ìœ¨: {statistics['mean_absolute_error']:.6f} ({statistics['mean_absolute_error_percent']:.3f}%p)")
        print(f"  ì¤‘ì•™ê°’ ì˜¤ì°¨ìœ¨: {statistics['median_absolute_error']:.6f} ({statistics['median_absolute_error_percent']:.3f}%p)")
        print(f"  í‘œì¤€í¸ì°¨: {statistics['std_error']:.6f} ({statistics['std_error_percent']:.3f}%p)")
        print(f"  ìµœì†Œ ì˜¤ì°¨: {statistics['min_error']:.6f} ({statistics['min_error']*100:.3f}%p)")
        print(f"  ìµœëŒ€ ì˜¤ì°¨: {statistics['max_error']:.6f} ({statistics['max_error']*100:.3f}%p)")
        print(f"  Q25: {statistics['q25_error']:.6f}")
        print(f"  Q75: {statistics['q75_error']:.6f}")
        print(f"\n[ìƒëŒ€ ì˜¤ì°¨]")
        print(f"  í‰ê·  ìƒëŒ€ ì˜¤ì°¨ìœ¨: {statistics['mean_relative_error_percent']:.2f}%")
        print(f"{'='*80}\n")
        
        # ì˜¤ì°¨ê°€ í° ìƒìœ„ 5ê°œ ìƒ˜í”Œ
        sorted_results = sorted(results, key=lambda x: x['error'], reverse=True)
        print(f"ì˜¤ì°¨ê°€ í° ìƒìœ„ 5ê°œ ìƒ˜í”Œ:")
        for i, r in enumerate(sorted_results[:5], 1):
            print(f"  {i}. ìƒ˜í”Œ #{r['index']}: ì‹¤ì œ={r['actual_rate']:.4f}, ì˜ˆì¸¡={r['predicted_rate']:.4f}, "
                  f"ì˜¤ì°¨={r['error_percent']:.2f}%p ({r['relative_error']:.1f}%)")
        
        # ì˜¤ì°¨ê°€ ì‘ì€ ìƒìœ„ 5ê°œ ìƒ˜í”Œ
        print(f"\nì˜¤ì°¨ê°€ ì‘ì€ ìƒìœ„ 5ê°œ ìƒ˜í”Œ:")
        sorted_best = sorted(results, key=lambda x: x['error'])
        for i, r in enumerate(sorted_best[:5], 1):
            print(f"  {i}. ìƒ˜í”Œ #{r['index']}: ì‹¤ì œ={r['actual_rate']:.4f}, ì˜ˆì¸¡={r['predicted_rate']:.4f}, "
                  f"ì˜¤ì°¨={r['error_percent']:.2f}%p ({r['relative_error']:.1f}%)")
        
        return {
            'statistics': statistics,
            'detailed_results': results,
            'test_data_path': test_data_path,
            'bin_width': bin_width
        }


def main():
    """ì‚¬ìš© ì˜ˆì‹œ"""
    print("=" * 80)
    print("TFT 4-Feature ëª¨ë¸ - ê°€ì¥ í™•ë¥ ì´ ë†’ì€ êµ¬ê°„ ì˜ˆì¸¡")
    print("=" * 80)
    
    predictor = ProbabilityPredictor(model_path='./results_tft_4feat/best_model.pt')
    
    # ì˜ˆì‹œ ì…ë ¥ê°’
    input_dict = {
        'ì˜ˆê°€ë²”ìœ„': 0.02,
        'ë‚™ì°°í•˜í•œìœ¨': 0.9,
        'ì¶”ì •ê°€ê²©': 53643620,
        'ê¸°ì´ˆê¸ˆì•¡': 48279258
    }
    
    print(f"\nì…ë ¥ í”¼ì²˜:")
    for key, value in input_dict.items():
        print(f"  {key}: {value}")
    
    # í™•ë¥ ì´ ë†’ì€ ìƒìœ„ 5ê°œ êµ¬ê°„
    result = predictor.get_highest_probability_ranges(input_dict, bin_width=0.001, top_k=5)
    
    print("\n" + "=" * 80)
    print(f"ëª¨ë¸ ì˜ˆì¸¡ ë²”ìœ„: {result['prediction_range']['min']*100:.2f}% ~ {result['prediction_range']['max']*100:.2f}%")
    print(f"ì¤‘ì•™ê°’: {result['statistics']['median']*100:.2f}%")
    print(f"í‰ê· : {result['statistics']['mean']*100:.2f}%")
    print("=" * 80)
    
    print("\n ì‚¬ì •ë¥ ì— ëŒ€í•œ êµ¬ê°„ë³„ í™•ë¥ ")
    print(f"\nâœ¨ í™•ë¥ ì´ ë†’ì€ ìƒìœ„ 5ê°œ êµ¬ê°„:")
    for i, r in enumerate(result['top_ranges'], 1):
        print(f"  {i}ìœ„. {r['range']} = ì‚¬ì •ìœ¨ {r['lower']*100:.1f}%~{r['upper']*100:.1f}% (í™•ë¥ : {r['probability_percent']:.2f}%)")
    
    # ì „ì²´ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•œ í‰ê°€ (10,000ê°œ ìƒ˜í”Œ)
    print("\n\n" + "=" * 80)
    print("ì „ì²´ í…ŒìŠ¤íŠ¸ ë°ì´í„° í‰ê°€ (ìµœëŒ€ 10,000ê°œ ìƒ˜í”Œ)")
    print("=" * 80)
    evaluation_result = predictor.evaluate_highest_probability_average(
        test_data_path='../dataset/dataset_feature_selected.csv',
        bin_width=0.001,
        max_samples=50000
    )


if __name__ == "__main__":
    print(f"Using device: {device}")
    main()
